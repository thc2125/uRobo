\documentclass[10pt, journal, compsoc]{IEEEtran}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{Tyrus~Cukavac%
\thanks{Special thanks to Homayoon Beigi for n-phone model inspiration and recommendation to compare against monophone model.}}
\title{uRobo: An End-to-End Concatenative Speech Synthesis System}
\date{December 21, 2017}
\begin{document}
\maketitle
\begin{abstract}
This paper introduces uRobo, an open concatenative speech synthesis system built using the Kaldi framework for its Automated Speech Recognition layer and utilizing deep recurrent neural networks for target feature predictions in the unit selection stage. uRobo is an end-to-end system, meant to automate construction of a final model from raw data to synthesized speech, and to produce limited synthetic human speech from a small initial transcribed audio data set. Included target feature prediction models for uRobo were trained on ten hours of the LibriSpeech audio corpus comprising multiple female voices, while units used for concatenation during testing came from one of two data sets: one comprised of 10 hours of heterogenous speaker data, and another consisting of roughly 25 minutes of data from a single speaker. Moreover, uRobo is capable of unit selection at the triphone level (with diphone and monophone backoff) as well as the monophone level. We show that uRobo is able to produce intelligible voices with both of these models, but the triphone-diphone-monophone backoff architecture produces more intelligible and slightly smoother results.
\end{abstract}
\section{Introduction}
The generation of artificial, yet natural-sounding speech is one of the oldest problems in computer science. Although production of a general, intelligible synthetic voice is a worthy goal, there also exists a need to create voices which sound like a target speaker. This need is particularly acute in the domains of healthcare and media production. Many people who must have their voiceboxes removed for medical reasons such as cancer, often wish for the ability to re-create their orginal voice. Moreover, for film production companies, imperfect audio captured during filming force them to engage in time-intense and expensive audio recording sessions with their talent in order to meet production deadlines.\par
The largest challenge in replicating a given voice is often the lack of quality, annotated vocal data from the target voice. Film productions may only have twenty to forty-five minutes of provided audio from a given voice. And when recreating a voice for medical purposes, an engineer is unlikely to find that their potential users have maintained extensive, transcribed recordings of their everyday conversations.\par
To this end, uRobo attempts to create a new speech synthesis model that sounds similar to a target voice but does not necessarily require the hours and hours of training data needed to produce full-fledged synthetic models.
\subsection{Existing Architectures}
State of the art text-to-speech architectures tend to fall into one of three categories: a parametric approach, an end-to-end neural network approach, and a concatenative approach.\par
The parametric architecture essentially works to make predictions on durations of phonemes as well as the acoustic features necessary for synthesis. These acoustic features are then provided to a vocoder which synthesizes the sounds into waveforms.TKTKThe parametric model described by Zen, et. al. utilizes an LSTM (Long Short Term Memory) RNN (Recurrent Neural Network) to make predictions of the above features to be provided to the vocoder.\cite{DBLP:journals/corr/ZenAEHS16} Although parametric models are resilient in the sense that they are capable of producing features for any given frame (in comparison to other models that may be limited by the unit data collected during trainign) the results are often apparently from a machine and lack a natural sound.\par
Parametric architectures need not be the only synthesis approach using neural networks. The most recent state of the art architectures for text-to-speech synthesis make use of neural networks to generate pure waveforms from input text. DeepMind's Wavenet uses dilated causal convolutional neural networks\cite{DBLP:journals/corr/ArikCCDGKLMRSS17}, and Baidu's Deep Voice passes inputs through layers of recurrent neural networks. Both of these approaches result in synthetic voices that human listeners seem to find more natural than previous approaches\cite{DBLP:journals/corr/OordDZSVGKSK16}, but require a great deal of computational power and time to train. Moreover, this area of study is still in relative infancy as opposed to other approaches, such as the concatenative, which have had more time to mature.\par
The concatenative approach, described by Hunt and Black, at its most basic level selects pre-existing units of audio from a database of utterances, stitching these together and performing basic post-processing to create a final audio output\cite{Hunt:1996:USC:1256383.1256532}. Obviously to provide adequate coverage for a given language and a given speaker, a great deal of audio data must be collected. This unfortunately leads to additional problems, such as slow database lookups for large candidate units for a given frame/phone. Google's advances in their own concatenative model have helped them address several of these issues by optimizing search for ideal unit candidates and extending the size of the units used to build utterances.\cite{45564}\par
Despite the above problems, the concatenative approach has yielded more natural-sounding results in many cases than the parametric approach, due to its use of existing waveforms rather than the synthesis of new waveforms by a vocoder, which often results in artifacts. For the purposes of imitating a given speaker, the concatenative approach may seemingly yield the best of all worlds: taking less time and resources than an end-to-end neural network while also being able to match actual phonetic level sounds of a given speaker. As a result, the uRobo system follows this architecture to synthesize speech.  
\section{The Concatenative Approach}
The concatenative approach to synthesis is at its heart a dynamic programming problem. Given a database of units which are in essence waveforms with acoustic features, the algorithm attempts to find a path/sequence of units corresponding to a given sequence of words/phonemes resulting in the minimum total cost value over all units. \par
$\overline{u}_1^n=min_{u_1,...,u_n}C(t_1^n,u_1^n)$\\
wherein $\overline{u}_1^n$ is the final sequence of $n$ units required for a new utterance that are selected by the algorithm. $u_x, x\in n$ is a given unit at index $x$ and $t_x, x\in n$ is a target "unit" or rather a representation of certain features that a unit at index $x$ should have. $C$ is a cost function over a sequence of units.\\	
The cost value per unit can be defined as a function of two separate costs. The target cost $C^t$, which is essentially a weighted sum of absolute differences between a target feature vector and a candidate unit's feature vector, as well as a concatenation cost $C^c$, which is the weighted sum of absolute differences between a candidate unit and a previous candidate.\par
$C^t(t_i,u_i)\sum_{j=1}^pw_j^t|t_{i_j}-u_{i_j}|$\\
$C^c(u_{i-1},u_i)\sum_{j=1}^qw_j^c|u_{i-1_j}-u_{i_j}|$\\
Where $p$ represents the number of features in the target feature vector and $q$ represents the number of features in the concatenation feature vectors of the two units. The cost for a given unit, then, is the sum of $C^c and C^t$. We can thus expand the cost of a sequence to be\\
$C(t_1^n,u_1^n)=\sum_{i=1}^nC^t(t_i,u_i)+ \sum_{i=1}^nC^c(u_{i-1},u_i)$
\cite{Hunt:1996:USC:1256383.1256532}\par
Obviously calculating this for every possible path would be an intractable problem, therefore the Viterbi algorithm is used to find the minimum cost sequence. The problem can be visualized as a sparse matrix wherein each column corresponds to the set of candidate costs for a given unit. The first column of the matrix is initialized as the concatenation cost between the first unit and silence. In the remaining iterations, unit by unit, each candidate is compared against the 1)target feature vector and 2) all previous units. The minimum concatenation cost is selected as the final concatenation cost for that unit, and the previous unit is stored in a backtracking matrix.
\begin{algorithm}
\begin{algorithmic}[1]
\Function{VITERBI-UNIT-COSTS}{$t_1^n,u_1^n$}
\State{matrix $cost$}
\State{matrix $back$}
\For{each $u_{1_x}$ in $u_1$}
    \State{$cost_{1,x}=C^c($SIL$,u_1)$}
\EndFor
\For{each $u_{i}$ in $u_2^n$}
    \For{each $u_{i_x}$ in $u_i$}
        \State{$C^t(t_i,u_{i_x})$}
        \State{$C^c(u_{i_x})=\min C^c(u_{i-1},u_i{_x})$}
        \State{$b=$argmin $C^c(u_{i-1},u_i{_x})$}
        \State{$cost_{i,x}=C^t(t_i,u_{i_x})+C^c(u_{i_x}+cost_{i-1,b}$}
        \State{$back_{i,x}=b$}
     \EndFor
\EndFor
\Return{$cost,back$}
\EndFunction
\end{algorithmic}
\end{algorithm}
Retrieving the final sequence of units is simple using a backtracing algorithm.
\begin{algorithm}
\begin{algorithmic}[1]
\Function{BACKTRACE-UNITS}{$cost,back$}
\State{$b=$argmin$cost_{n}$}
\State{$\overline{u}_n=b$}
\For{each $i\in (n-1) \to 1)$}
    \State{$b=back_{i+1,b}$}
    \State{$\overline{u}_i=b$}
\EndFor
\Return{$\overline{u}_1^n$}
\EndFunction
\end{algorithmic}
\end{algorithm}
This final sequence of units i.e. selected waveforms, are then stitched together to create the final waveform representing an utterance.
\section{uRobo Architecture}
The uRobo system is a concatenative system based on both monophones and/or triphones with diphone and monophone backoff, to be discussed in another section. To that end, the system is composed of four distinct layers whose ultimate output combines to produce a speech synthesis model. First, the system needs to be able to decompose audio in a manner that features can be extracted for use in unit selection, and therefore requires a speech recognition system in order to produce data usable by the concatenative architecture. Next, a system is required to pre-process the raw output of the ASR in order to extract additional features and put them into a form that the target feature predicter can learn from. Next, a model for predicting target features must be trained on the pre-processed data. Finally, a unit selection system using the viterbi algorithm and either the previously extracted data or some other pre-processed data can be used to acquire units and ultimately synthesize an audio file from text.
\subsection{Speech Recognition System}
For automated speech recognition, the Kaldi ASR framework was utilized. Kaldi has pre-built scripts that allow a user to train an ASR system on the LibriSpeech corpus\cite{unknown}, an open corpus that is the foundation of the data used by the concatenation synthesis system. Kaldi's training script for ASR downloads the LibriSpeech corpus as well as the language model files generated from an analysis of that corpus, i.e. vocabulary, phones, and a lexicon translating words to various phone sequences.\par
Kaldi then goes through all of the data, a total of 1000 hours, and builds triphone based alignment models utilizing HMM-GMMs and decision trees, extracting features such as MFCCs as well as performing multiple alignments on the data to improve each successive model.\par
with a final alignment model completed, the uRobo system can call kaldi scripts to perform forced alignment on the librispeech data. For the purposes of development and the experiments to follow, the clean training data of Librispeech, comprising 100 hours of audio data, as well as the clean test data, were aligned and output into a text format that specified a given utterance's timecode to specific phones. \par
Of particular use in the uRobo system was Kaldi's generation of an alignment lexicon, which provided additional contextual information for all of the possible/available phones, such as phone placement within the word. This became especially important for reducing the number of candidate units to select from during unit selection.
\subsection{Preprocessing}
Given properly aligned data from Kaldi's forced alignment, it now needed to be converted into a format usable to uRobo. The system itself required a target feature prediction system (to be discussed in the next section). To train such a system, the output from Kaldi needs to be re-analyzed and converted to more digestible formats. The preprocessor is able to take librispeech audio data and convert into wav format (for standardized feature extraction) as well as convert other Kaldi provided metrics and annotations for use in the training/synthesis stage. The preprocessor also allows for segmentation of the data set by gender, speaker, and total duration.\par
First, the preprocessor reads in Kaldi's "wav.scp", "spk2gender", "phones", "words", and "align\_lexicon.txt" files, converting them to Python dictionaries and storing them in the json format for easier and universal ingestion. Then, based on the filter criteria of speaker, duration, and gender, the preprocessor selects utterances from Librispeech to comprise the pre-processed data.\par
Of key importance to the Librispeech model is the triphone/diphone/monophone chunking when analysing textual data. To this end, in order to train and utilize a target feature prediction model, a numerical index must be assigned to each monophone, diphone, and triphone respectively. Because modeling all possible triphones and diphones is virtually intractable, the utterances are decomposed into diphones and triphones. Diphones and triphones that make up $1\%$ and $.1\%$ of the total data set respectively, are considered "representative" diphones or triphones and assigned a numerical index. A final index list of monophones, diphones and triphones is then stored. (Note that this list of monophones, diphones, and triphones can later be used during preprocessing of other data to ensure uniformity of index assignment across different sets of utterances.)\par
With this list of n-phones, the utterances are then analyzed and chunked. The basic scheme for index assignment is triphones with diphone and monophone backoff, and an overlap of a single phone for each multi-phone chunk.\par
That is, for each phone $p_i$ in each utterance, the following triphone $p_i,p_{i+1},p_{i+2}$,is determined. If the triphone is representative (i.e. an index exists for it in the monophone-diphone-triphone dictionary previously extracted), the triphone's index is assigned to this chunk. If the triphone is not representative, however, the following diphone $p_i,p_{i+1}$ is determined instead. If it is representative, then an index is assigned to this chunk. If it is not, then the monophone is determined and its index is assigned to the phone. In all cases, the following phone to be chunked in this manner is the final phone in the current triphone ($p_{i+2}$) or diphone ($p_{i+1}$). This creates a single phone overlap for each chunk in an utterance. In the case of a monophone index being assigned, the following phone $p_{i+1}$ is chunked next.\par
Given the monophone-diphone-triphone index representation of an utterance, features need to be extracted both for determining the target cost $C^t$ and the concatenation cost $C^c$. uRobo's target feature prediction model, discussed in the next subsection, attempts to predict acoustic features and compare against units in the database. Acoustic features selected, based on previous work in DNN target feature prediction are a given nphone's duration, energy, initial phone $f_0$ and final phone $f_0$. (Obviously in the case of monophones, initial phone $f_0$ and final phone $f_0$ are the same.) Energy in this instance is the sum of squared signal values divided by the duration of the total unit (for $n$ samples $s_{1...n}$, $\frac{\sum_{j=1}^n s_j^2}{n}$).\cite{Jurafsky:2009:SLP:1214993}. Because our final synthesis will consist of overlapping phones, we are interested in similar phone-level features. In comparison to other models, which typically extract features on adjoining frames during concatenation\cite{a102c4924c19470ab180d278d2029de5}, our concatenation cost will make use of the existing target features for overlapping phones.\par
Because the features are meant to be fed as training output for a neural network, and given the wildly differing ranges of features, feature scaling must also occur in the pre-processing stage. Each feature is standardized against the mean and standard deviation over the utterances by any given speaker, so that each feature's final score is speaker independent. So for a target feature vector $t_{s}$ emitted by speaker $s$, $\hat{t}_s=\frac{t_s-\mu_s}{\sigma_s}$\cite{Beigi:2011:FSR:2124400}. In this manner, the target feature prediction model can be trained over significant amounts of data from different speakers while still being useful in predicting acoustic features for a single speaker.\par
For a given data set, feature extraction occurs at the phone level as well, in order to provide for more phonetic coverage in the unit database: whereas with the n-phone model alone, many phones are incorporated into either tri-phones or di-phones, by having each utterance also chunked at the phone level, more units are available to each of the initially identified phones.
\subsection{Target Feature Prediction Using a Deep Recurrent Neural Network}
Hunt and Black present the general concept behind the Concatenative approach, but there is still the question of how to generate target features for computation of the target cost $C^t$. Much work has been done in this area, ranging from Festival's Multisyn system, which primarily utilizes linguistic features (phone, word, syllable boundaries, and accents)\cite{a102c4924c19470ab180d278d2029de5} as well as optional pitch contour and segment durations to create a target sequence of units that are then compared against the database. More recent research has begun to explore the potential of deep neural networks for predicting target features, learning both useful linguistic features and their association with desired acoustic features simultaneously. Merritt, et. al. utilized 5-6 layer feed forward DNNs to guide unit selection within the context of the Festival framework.\cite{7472658}.\par
For the uRobo architecture, a Recurrent Neural Network (RNN) model was adopted, inspired by that described by Fernandez, et. al. from the IBM Haifa Research Lab. The benefits of using a recurrent model are manifold, in particular the ability of RNNs to learn contextual information from given sequences. The base model consists of three stacked Bidirectional Long Short-Term memory (LSTM) layers, which have the added benefit of retaining useful information throughout both forward and backward runs through a phone sequence.\cite{unknowndnn} For the purposes of this prototype, Tensorflow utilizing a Keras front-end was used to construct and train the neural mode. \par
The input to the neural model consisted of indexes referencing n-phones as defined previously. These were then translated to a neural embedding layer that was learned along with the phonetic-acoustic model. The uRobo model learns 32-dimensional embeddings for each given n-phone index. 
\par As per standard LSTM architecture, each n-phone embedding for a given utterance is then fed into a recurrent cell in the LSTM, the output of which was fed as input back into the recurrent cell (along with the next n-phone embedding). Because the layer is bidirectional, each of the n-phones are then input backwards through the recurrent pipeline (to acquire reverse context for each phone as well), with outputs at each stage of this sequence concatenated with the outputs of the forward layer. These final concatenated outputs (which are one-to-one with the phone inputs) are then used as sequential inputs into another bidirectional LSTM layer. The three Bidirectional LSTM layers output vectors of dimensions 67, 57, and 46 respectively.\cite{unknowndnn} A final dense layer is then distributed over the entire sequence, producing a 4-dimensional target feature vector for each given phone.
Each model is trained using the mean square error as the loss function. Although the model on which the predicter is based utilized standard stochastic gradient descent\cite{unknowndnn} for back propagation, initial training test demonstrated slow, if any, convergence. Therefore for the purposes of our experiments, the RMS Prop optimizer was employed.\par
\subsection{Unit Selection and Concatenation}
With a model capable of predicting target features, all that remains is to select audio units at the n-phone level to produce the final synthetic audio. For testing purposes, uRobo is capable of concatenating both n-phones as defined previously or monophones, if specified by the user.\par 
Given a sequence of words $w$ to synthesize, a sequence of $m$ n-phones $p_1^m$ is generated, using a naive and direct translation from word to phone sequence as provided by Kaldi's alignment lexicon. Given the high number of silence phones found within the audio data, adding silence was shown to have a detrimental effect on both runtime and fluency/intelligibility. Therefore silence phones were ignored in phone sequence generation. The phones are chunked in a manner similar to initial preprocessing.\par 
With $p_1^m$, candidate pre-selection occurs to significantly reduce the unit search space for each n-phone. This is especially useful for data sets with many hours of audio, as the Viterbi algorithm utilized for unit selection runs in $O(n^3)$ time. Pre-selection in this instance follows a naive "phone to units" scheme, where for any given n-phone, the only possible candidate units are those that were classified as that specific n-phone during pre-processing. If no candidate units for an n-phone exist, the synthesizer splits the n-phone back into monophones, and selects candidate units from the monophone data that was also collected during pre-processing. Following candidate selection, the system uses the target feature prediction neural network to generate a sequence of target features for each phone.\par 
Costs are now calculated for each candidate unit as defined in section 2, with the caveat that the initialization of the cost matrix, does not determine a candidate unit's cost based on concatenation cost of the unit with silence ($C^c($SIL$u_1$). Instead, the cost is purely a function the candidate unit's target cost.\par
To encourage selection of overlapping units, a concatenation cost of 0 is applied if two adjacent candidate units do in fact overlap in the source data.\par
When final units are selected from the data, each of the units is concatenized, albeit with the last phone of a given unit overlapping with the first phone of the next unit. To make transitions smoother and avoid the unpleasant effect of hearing multiple voices at once, an exponential cross fade is applied at the time of the join, i.e. the first sample of the final phone in the previous unit. Note that if the concatenative model being used solely uses monophones as units, the units are simply concatenated together. A final wav file is exported by the system.
\section{Experiment Setup}
\subsection{The Corpus}
As previously mentioned, much of the difficulty in speech synthesis lies in a lack of substantial and usable training data. Many of the corpora used by large corporations incorporate many hours of audio data recorded by professional voice actors, and are not freely available to the public. The open Librispeech corpus\cite{unknown} aims to fill this gap by formatting and segmenting data from the LibriVox project, providing thousands of hours of transcribed audio data for use in speech tasks. Unfortunately, each speaker in the corpus tends to only contribute a maximum of around 30 minutes of audio to the overall project, meaning any attempts at making use of this data for a text-to-speech model will either need to compose units from varied voices or from a very limited data set of audio.\par
\subsection{Utterance Synthesis and Metrics}
Both of the above approaches were used in the experiments. A set of ten utterances were generated. Each word conatined one to five syllables, in increasing order. The five sentences likewise began simply but became progressively more difficult to pronounce. The utterances were generated by synthesizers using two different data sets. One utilized a single female speaker from the LibriSpeech training corpus, comprising roughly 25 minutes of audio spoken. The other, to test the viability of "found data" or combinations of pre-existing but sonically distinct data, used ten hours of total audio spoken by multiple female readers from the corpus. To test the variation between triphone with backoff and pure monophone unit selection, the single speaker synthesizer followed both the n-phone with backoff model and the monophone model. This resulted in 3 different synthetic voices. All of the test voices utilized the same target feature prediction model, trained on ten hours of female speaker data.\par 
The target words/sentences themselves were selected from the test set of the Librispeech corpus. This assured, at least at the sentence level, that the utterances would be unseen by any of the voices. This selection had the added benefit of providing pre-recorded human audio so that a control group could be used as a baseline comparison for the desired metrics. \par
The generated and control audio was judged by a blind sampling of five Mechanical Turk workers per utterance. Each judged an individual audio file in isolation from any of the others based on three metrics: the intelligibility of the utterance, how human it sounded, and the smoothness of the utterance.\par
Although a similarity test comparing the synthesized audio with the original speaker would provide insight into uRobo's effectiveness at creating a voice similar to a target, this would likely be better performed with a new corpus and a new speaker with more than thirty minutes of data. As it is, synthesized speech matching sentences previously uttered by the initial speaker are very apparently from that speaker due to the zero cost concatenation between overlapping n-phones. Moreover, because the phonetic coverage of only 25 minutes of data is limited as is (there are still phones in the index without unit representation), all units from the initial 25 minutes need to be utilized in production of the audio.
\section{Results}
Fig. 1 shows the results of the vocal quality survery given over mechanical turk. A total of 28 different workers participated in the survey and each judged an average of roughly 7 voice samples. The top four workers each listened to more than 27 of the 40 samples provided. \par
The results demonstrate leadership of the control, purely human voice, which is to be expected. Similarly, the single speaker voice utilizing the n-phone model also shows dominance over both the multi-speaker voice and the monophone only single speaker voice.\par
\begin{figure}[h]
\caption{In the figure below, "c" represents the control human voice, "t10f" represents the voice selecting from ten hours of multi-speaker data, "ts39" refers specifically to the single speaker voice (speaker 39 in LibriSpeech) and "ts39\_m" represents its monophone variant.}
\centering
\includegraphics[scale=0.5]{../experiments/results/results_graph.png}
\end{figure}
The high variability in the answers as demonstrated by the standard deviation line showcases a similar trend, but also the difficulty in accurately measuring the effectiveness of a given voice. That said, clear trends do emerge, and the results clearly show the superiority of a triphone model with backoff over a pure monophone model. 
\section{Conclusions and Avenues for Future Work}
There are a number of areas for improvement in the current architecture. In particular, with regards to being able to "re-synthesize" the original audio given a string of text. The phonetic pronunciation of a given sentence is provided without respect to the placement of silences. The original LibriSpeech data set is rife with silences between some words but not others, and also at the beginning and end of each utterance. uRobo currently uses the alignment lexicon to translate a word into its phonetic pronunciation, without regard to any intermediate silences that might occur. For a future version of uRobo, it may be worthwhile to experiment with a sequence to sequence model or HMM that is capable of learning when and where silences should be placed. This might allow for better unit selection, particularly with low resource speakers that may have many triphone and diphone sequences that are dependent on the silences between words.\par
Additionally, multiple other experiments may be useful in determining the viability of the uRobo model in single speaker voice simulation. Although the scope of this current project focused specifically on thirty minutes or less of a single speaker's data, it would be interesting to compare current results with a significantly larger single speaker corpus, possibly as a means of tuning hyperparameters for the neural model and experimentation of different feature sets for calculating concatenation and target cost.\par
Tests against the ten hour multi-speaker data set proved to significantly slower than real time when synthesizing simple sentences, due to the inherently high time complexity of the Viterbi algorithm as well as the cursory unit preselection. Many other models utilize Hidden Markov Model-Gaussian Mixture Model decision trees to carry out pre-selection.\cite{45564} This approach would certainly be worth investigating should larger unit data sets prove a fruitful avenue of inquiry.
\bibliographystyle{IEEETran}
\newpage
\bibliography{thc2125_paper-ROUGH1}

\end{document}
